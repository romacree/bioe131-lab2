{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 100% 0's\n",
    "bitfile100 = np.random.choice([0,1], size = 838860800, replace = True, p = [1,0])\n",
    "bitfile100 = np.packbits(bitfile100)\n",
    "open(\"bitfile100\", \"wb\").write(bitfile100)\n",
    "\n",
    "# 90% 0's\n",
    "bitfile90 = np.random.choice([0,1], size = 838860800, replace = True, p = [.9,.1])\n",
    "bitfile90 = np.packbits(bitfile90)\n",
    "open(\"bitfile90\", \"wb\").write(bitfile90)\n",
    "\n",
    "\n",
    "# 80% 0's\n",
    "bitfile80 = np.random.choice([0,1], size = 838860800, replace = True, p = [.8,.2])\n",
    "bitfile80 = np.packbits(bitfile80)\n",
    "open(\"bitfile80\", \"wb\").write(bitfile80)\n",
    "\n",
    "\n",
    "\n",
    "# 70% 0's\n",
    "bitfile70 = np.random.choice([0,1], size = 838860800, replace = True, p = [.7,.3])\n",
    "bitfile70 = np.packbits(bitfile70)\n",
    "open(\"bitfile70\", \"wb\").write(bitfile70)\n",
    "\n",
    "\n",
    "# 60% 0's\n",
    "bitfile60 = np.random.choice([0,1], size = 838860800, replace = True, p = [.6,.4])\n",
    "bitfile60 = np.packbits(bitfile60)\n",
    "open(\"bitfile60\", \"wb\").write(bitfile60)\n",
    "\n",
    "\n",
    "# 50% 0's\n",
    "bitfile50 = np.random.choice([0,1], size = 838860800, replace = True, p = [.5,.5])\n",
    "bitfile50 = np.packbits(bitfile50)\n",
    "open(\"bitfile50\", \"wb\").write(bitfile50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnastring = np.random.choice(['A','G','C','T'], size = 100000000, replace = True)\n",
    "dnastringfile = open(\"dnastring.fa\", \"w\").write(\"\".join(dnastring))\n",
    "\n",
    "aastring = np.random.choice(['R','H','K','D','E','S','T','N','Q','C','G','P','A','I','L','M','F','W','Y','V'],\n",
    "                            size = 100000000, replace = True)\n",
    "aastringfile = open(\"aastring.fa\",\"w\").write(\"\".join(aastring))\n",
    "\n",
    "                            \n",
    "                            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Compression Table\n",
    "| File name     | initial file size | gzip time  | gzip file size  | bzip2 time | bzip2 file size | pbzip time | pbzip file size | ArithmeticCompress time | ArithmeticCompress file size\n",
    "| ------------- |:-------------:| :-----:|:----:|:------:|:------:|:------:|:--------:|:-------:|--------:|\n",
    "| bitfile100    | 105 MB  | 0m0.719s | 102 kB | 0m1.043s | 113 B | 0m0.125s | 5.62 kB | 0m0.132s | 1.03 kB |\n",
    "| bitfile90     | 105 MB  | 0m18.920s | 58.7 MB | 0m10.674s | 61.2 MB |  0m0.753s | 61.2 MB | 0m0.332s | 49.2 MB |\n",
    "| bitfile80     | 105 MB  | 0m13.371s | 81.2 MB| 0m11.979s | 86.6 MB | 0m0.942s | 86.7 MB | 0m0.248s | 75.7 MB |\n",
    "| bitfile70     | 105 MB  | 0m6.011s | 93.6 MB |  0m13.838s | 99.8 MB | 0m1.121s | 99.8 MB | 0m0.297s | 92.4 MB |\n",
    "| bitfile60     | 105 MB  | 0m4.284s | 102 MB | 0m15.731s | 105 MB | 0m1.380s | 105 MB | 0m0.376s | 102 MB |\n",
    "| bitfile50     | 105 MB  | 0m3.531s | 105 MB | 0m16.713s | 105 MB | 0m1.477s | 105 MB | 0m0.372s | 105 MB |\n",
    "| dnastring.fa     | 100 MB  | 0m12.143s | 29.2 MB | 0m9.481s | 27.3 MB | 0m0.690s | 27.3 MB | 0m0.208s | 25 MB |\n",
    "| aastring.fa     | 100 MB  | 0m4.256s | 60.6 MB | 0m10.012s | 55.3 MB | 0m0.779s | 55.3 MB | 0m0.304s | 54 MB |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-real-data-compression Questions\n",
    "### Which algorithm achieves the best level of compression on each file type?\n",
    "\n",
    "**On bitfiles:**\n",
    "With the exception of bzip on bitfile100, ArithmeticCompress is the best algorithm for compression out of the four tested\n",
    "\n",
    "**On fasta files:**\n",
    "Similar to bitfiles, ArithmeticCompress is the best algorithm for compression out of the four tested, yielding a 75% decreases in file size for the dnastring file as opposed to the <75% decreases in file size for the other three algorithms, and ~50% decrease in file size for the aastring file as opposed to <~50% decrease in file size for the other three algorithms\n",
    "### Which algorithm is the fastest?\n",
    "\n",
    "The fastest is ArithmeticCompress for all files\n",
    "\n",
    "\n",
    "### What is the difference between bzip2 and pbzip2? Do you expect one to be faster and why?\n",
    "\n",
    "\n",
    "### How does the level of compression change as the percentage of zeros increases? Why does this happen?\n",
    "The amount of compression for each compression type (gzip, bzip, pbzip, ArithmeticCompress) increases as the percent of 0's in the file increases. \n",
    "### What is the minimum number of bits required to store a single DNA base?\n",
    "\n",
    "2 bits (01, 10, 11, 00)\n",
    "\n",
    "### What is the minimum number of bits required to store an amino acid letter?\n",
    "\n",
    "4.32 bits (log20/log2) theoretically, 5 bits practically.  6 bits per codon (2 bits per base, 3 bases per amino acid letter)\n",
    "\n",
    "### In your tests, how many bits did gzip and bzip2 actually require to store your random DNA and protein sequences?\n",
    "\n",
    "**For gzip:**\n",
    "DNA Sequence Fasta file: 29.2 MB = 29,200,000 bytes = 233,600,000 bits\n",
    "Protein Sequence Fasta file: 60.6 MB = 60,600,000 bytes = 484,800,000 bits\n",
    "\n",
    "**For bzip2:**\n",
    "DNA Sequence Fasta file: 27.3 MB = 27,300,000 bytes = 218,400,000 bits\n",
    "Protein Sequence Fasta file: 55.3 MB = 55,300,000 bytes = 442,400,000 bits\n",
    "\n",
    "### Are gzip and bzip2 performing well on DNA and proteins?\n",
    "They perform well enough, reducing ~3/4 of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import Entrez\n",
    "from Bio import SeqIO\n",
    "\n",
    "Entrez.email = 'romacree@berkeley.edu'\n",
    "ids = []\n",
    "handle = Entrez.esearch(db = 'nucleotide', term = \"gp120\", sort = 'relevance', idtype = 'acc')\n",
    "for i in Entrez.read(handle)[\"IdList\"]:\n",
    "    handle = Entrez.efetch(db = 'nucleotide', id = i, rettype = 'fasta', retmode = 'text')\n",
    "    ids.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AF236860.1', 'AF236859.1', 'Y14421.1', 'Y14420.1', 'Y14419.1', 'Y14418.1', 'Y14417.1', 'Y14416.1', 'Y14415.1', 'Y14414.1', 'Y14413.1', 'Y14412.1', 'Y14411.1', 'Z97169.1', 'Z97168.1', 'Z97167.1', 'Z97166.1', 'Z97165.1', 'Z97164.1']\n"
     ]
    }
   ],
   "source": [
    "# To make a list of 10 ID's\n",
    "print(ids)\n",
    "pop = 0\n",
    "while pop <= 8:\n",
    "    ids.pop()\n",
    "    pop+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntseqs = []\n",
    "for accession in ids:\n",
    "    record = Entrez.efetch(db = 'nucleotide', id = accession, rettype = 'gb', retmode = 'text')\n",
    "    seq = SeqIO.read(record,'genbank')\n",
    "    ntseqs.append(str(seq.seq))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import SeqIO\n",
    "gp120file = open(\"gp120-homologs.fa\",\"w\")\n",
    "\n",
    "for seq in ntseqs:\n",
    "    gp120file.write(seq + \"\\n\")\n",
    "    \n",
    "gp120file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subsequent manipulation to .fa file done in terminal to include headers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A priori, do you expect to achieve better or worse compression here than random data? Why?\n",
    "We would expect better compression than for random data because there is less \"information\" present in the more structured DNA sequences, based off of thermodynamic and evolutionary constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Given the benchmarking data you obtained in this lab, which algorithm do you propose to use for each type of data? Provide an estimate for the fraction of space you can save using your compression scheme. How much of a bonus do you anticipate receiving this year?\n",
    "\n",
    "**For the 80% of data being resequencing of genomes and plasmids:**\n",
    "\n",
    "This is effectively the same as the gp120-homologs.fa compression we did earlier, such that a compression algorithm which deals best wi\\\\ this sort of data will be used. gzip will be used for this as it yielded the best compression ratio (.23)\n",
    "\n",
    "**For the 10% of data being protein sequences:**\n",
    "\n",
    "We can use ArithmeticCompress for the protein sequence files, as this yielded the highest reduction in file size in the aastring file example.\n",
    "\n",
    "**For the 10% of data being binary microscope image data (random)**:\n",
    "\n",
    "We can use any of the algorithms for completely random data, as the compression efficiency did not vary between the algorithms for bitfile50 (which is the completely random binary file). For the sake of speed, we will again use ArithmeticCompress as this compressed the file (albeit uneffectively) in the shortest amount of time.\n",
    "\n",
    "**Bonus calculations:**\n",
    "\n",
    "**For 80% data:**\n",
    "\n",
    "Compression ratio = 0.23\n",
    "(0.80 x .23 x 100) = 18.4% of data remaining in that grouping. This is an (100-18.4%) = 81.6% reduction in\n",
    "the amount of data.\n",
    "\n",
    "Amount savings: ($50 / terabyte)(1000 terabytes/day)(365days/year) [(1-.23 compression rate)(.8) + (.46)(.1) + (0)(.1)]  = $12,081,500* bonus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| File name     | initial file size | gzip time  | gzip file size  | bzip2 time | bzip2 file size | ArithmeticCompress time | ArithmeticCompress file size\n",
    "| ------------- |:-------------:| :-----:|:----:|:------:|:------:|:------:|--------:|\n",
    "| gp120-homologs.fa    | 5.48 kB | 0m0.003s | 1.26 kB | 0m0.005s | 1.33 kB | 0m0.009s | 2.45 kB |\n",
    "\n",
    "\n",
    "**Compression Ratios:**\n",
    "\n",
    "\n",
    "gzip: 1.26/5.48 = **.23**\n",
    "\n",
    "\n",
    "bzip2: 1.33/5.48 = **.24**\n",
    "\n",
    "\n",
    "ArithmeticCompress: 2.45/5.48 = **.45**\n",
    "\n",
    "Compared to compression of random data in \"dnastring.fa\" in which compression ratios for gzip, bzip2, and ArithmeticCompress is .292, .273, .25, respectively, gzip and bzip2 compress better for \"gp120-homologs.fa\" but ArithmeticCompress does not, with a compression rate .2 worse than in the random data\n",
    "\n",
    "\n",
    "\n",
    "## Given the benchmarking data you obtained in this lab, which algorithm do you propose to use for each type of data? Provide an estimate for the fraction of space you can save using your compression scheme. How much of a bonus do you anticipate receiving this year?\n",
    "\n",
    "##### For the 80% of data being resequencing of genomes and plasmids:\n",
    "\n",
    "This is effectively the same as the gp120-homologs.fa compression we did earlier, such that a compression algorithm which deals best with this sort of data will be used. gzip will be used for this as it yielded the best compression ratio (.23)\n",
    "\n",
    "##### For the 10% of data being protein sequences:\n",
    "\n",
    "We can use ArithmeticCompress for the protein sequence files, as this yielded the highest reduction in file size in the aastring file example.\n",
    "\n",
    "##### For the 10% of data being binary microscope image data (random):\n",
    "\n",
    "We can use any of the algorithms for completely random data, as the compression efficiency did not vary between the algorithms for bitfile50 (which is the completely random binary file). For the sake of speed, we will again use ArithmeticCompress as this compressed the file (albeit uneffectively) in the shortest amount of time.\n",
    "\n",
    "\n",
    "##### Bonus calculations:\n",
    "\n",
    "##### For 80% data: Use algorithm that was best with gp120-homologs.fa which is gzip (77% file size reduction)\n",
    "\n",
    "##### For 10% protein sequences: Use algorithm that was best with aastring.fa which is ArithmeticCompress (46% file size reduction)\n",
    "\n",
    "##### For 10% image data: Use algorithm best with bitfile50 which was ArithmeticCompress (fastest but no algorithms reduced the file size so 0% file size reduction)\n",
    "\n",
    "##### Our compression scheme will save\n",
    "[(1-.23)(.8) + (1-.54)(.1) + (0)(.1)] = .662 = 66.2% space saved\n",
    "\n",
    "##### Amount savings: \n",
    "(50 dollars/terabyte)(1000 terabytes/day)(365 days/year)[] = $12,081,500 saved per year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
